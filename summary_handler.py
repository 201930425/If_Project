import torch
import traceback
import threading
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from db_handler import fetch_data_from_db
from config import KOBART_MODEL_NAME
import math

# --- KoBART ëª¨ë¸ ìƒíƒœ ë³€ìˆ˜ ---
kobart_model = None
kobart_tokenizer = None
kobart_loading = False
latest_summary = "[ìš”ì•½ì€ 'ìš”ì•½ ë³´ê¸°'ë¥¼ ëˆ„ë¥´ì„¸ìš”]"
DEVICE = "cpu"  # â­ï¸ KoBARTëŠ” CPUë¡œ ì‹¤í–‰ (VRAM ë¶€ì¡±)


# -----------------------------

def load_kobart_model():
    """KoBART ëª¨ë¸ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    global kobart_model, kobart_tokenizer, kobart_loading, latest_summary, DEVICE
    if kobart_model is None and not kobart_loading:
        kobart_loading = True
        latest_summary = "[KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...]"  # ìƒíƒœ ì—…ë°ì´íŠ¸

        # â­ï¸ (VRAM 2GBë¡œëŠ” GPU ê°€ì† ì‹¤íŒ¨)
        DEVICE = "cpu"
        print(f"ğŸ”„ KoBART ëª¨ë¸ ë¡œë“œ ì¤‘... (Device: {DEVICE})")

        try:
            kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained(
                KOBART_MODEL_NAME,
                ignore_mismatched_sizes=True
            )
            kobart_model = BartForConditionalGeneration.from_pretrained(
                KOBART_MODEL_NAME,
                ignore_mismatched_sizes=True
            )
            kobart_model.to(DEVICE)  # â­ï¸ CPUë¡œ ì„¤ì •

            print(f"âœ… KoBART ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. (Device: {DEVICE})")
            latest_summary = "[ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ìš”ì•½ ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš”]"
            return True
        except Exception as e:
            print(f"âš ï¸ KoBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            kobart_model = None
            kobart_tokenizer = None
            latest_summary = "[ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”]"
            return False
        finally:
            kobart_loading = False


# â­ï¸ í—¬í¼ í•¨ìˆ˜: ì‹¤ì œ ìš”ì•½ ì‹¤í–‰ê¸° (íŒŒë¼ë¯¸í„°í™”)
def _summarize_internal(text_chunk, max_gen_len=150, min_gen_len=30):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì¡°ê°(chunk)ì„ ì§€ì •ëœ ê¸¸ì´ë¡œ ìš”ì•½í•©ë‹ˆë‹¤."""
    global kobart_tokenizer, kobart_model, DEVICE

    try:
        # <s>, </s> íƒœê·¸ ì¶”ê°€
        text_with_tags = '<s>' + text_chunk + '</s>'

        inputs = kobart_tokenizer(
            text_with_tags,
            return_tensors="pt",
            max_length=1024,  # â­ï¸ ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´
            truncation=True,
            padding="max_length"
        )

        # â­ï¸ ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ë™ì¼í•œ ì¥ì¹˜ë¡œ ì´ë™
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

        summary_ids = kobart_model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            num_beams=4,
            max_length=max_gen_len,  # â­ï¸ ê°€ë³€ ê¸¸ì´ ì ìš©
            min_length=min_gen_len,  # â­ï¸ ê°€ë³€ ê¸¸ì´ ì ìš©
            early_stopping=True,
            no_repeat_ngram_size=2
        )

        summary_raw = kobart_tokenizer.decode(summary_ids[0])
        summary_cleaned = summary_raw.replace('<s>', '').replace('</s>', '').replace('<usr>', '').strip()

        return summary_cleaned

    except Exception as e:
        print(f"âš ï¸ ìš”ì•½(ë‚´ë¶€) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return "[ìš”ì•½ ì¡°ê° ìƒì„± ì‹¤íŒ¨]"


# â­ï¸ [ìˆ˜ì •] Map-Reduce ë¡œì§ + ë‹¨ì¼ ì²­í¬ ìµœì í™” + ê¸¸ì´ ì˜µì…˜
def summarize_text(text, length_mode="medium"):
    """
    KoBART ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    length_mode: 'short', 'medium', 'long'
    """
    global kobart_tokenizer, kobart_model
    if not text.strip():
        return "[ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤]"
    if kobart_model is None or kobart_tokenizer is None:
        load_kobart_model()
        if kobart_model is None:
            return "[KoBART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]"

    print(f"ğŸ”„ ìš”ì•½ ì‘ì—… ì‹œì‘... (ëª¨ë“œ: {length_mode})")

    # â­ï¸ 1. ëª©í‘œ ìš”ì•½ ê¸¸ì´ ì„¤ì •
    if length_mode == "short":
        final_max = 100
        final_min = 20
    elif length_mode == "long":
        # A4 ìš©ì§€ 1ì¥ ëª©í‘œ (ì•½ 1000í† í°)
        final_max = 1000
        final_min = 600
    else:  # medium
        final_max = 250
        final_min = 50

    sentences = [s.strip() for s in text.split('\n') if s.strip()]
    if not sentences:
        return "[ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤]"

    # â­ï¸ 2. Map ë‹¨ê³„: ì²­í¬í™”
    max_chunk_tokens = 1000
    current_chunk_sentences = []
    current_chunk_tokens = 0
    intermediate_summaries = []

    print(f" (1/3) ì´ {len(sentences)}ê°œ ë¬¸ì¥ ì²­í¬í™” ì‹œì‘...")

    for sentence in sentences:
        sentence_tokens = len(kobart_tokenizer.tokenize(sentence))

        if current_chunk_tokens + sentence_tokens > max_chunk_tokens:
            # ì²­í¬ê°€ ê½‰ ì°¼ìœ¼ë©´ 'ì¤‘ê°„ ìš”ì•½' ì‹¤í–‰ (Map)
            # ì¤‘ê°„ ìš”ì•½ì€ ì •ë³´ ì†ì‹¤ì„ ë§‰ê¸° ìœ„í•´ ì ë‹¹í•œ ê¸¸ì´(150) ìœ ì§€
            if current_chunk_sentences:
                chunk_text = " ".join(current_chunk_sentences)
                chunk_summary = _summarize_internal(chunk_text, max_gen_len=150, min_gen_len=30)
                intermediate_summaries.append(chunk_summary)

            current_chunk_sentences = [sentence]
            current_chunk_tokens = sentence_tokens
        else:
            current_chunk_sentences.append(sentence)
            current_chunk_tokens += sentence_tokens

    # â­ï¸ 3. ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬ (ì¤‘ìš” ìˆ˜ì •)
    if current_chunk_sentences:
        chunk_text = " ".join(current_chunk_sentences)

        # â­ï¸ [í•µì‹¬ ìˆ˜ì •] ë§Œì•½ ì´ê²ƒì´ 'ì²« ë²ˆì§¸ì´ì ë§ˆì§€ë§‰' ì²­í¬ë¼ë©´ (ì¦‰, ì „ì²´ í…ìŠ¤íŠ¸ê°€ í•œ ë²ˆì— ë“¤ì–´ê°„ë‹¤ë©´)
        # ì¤‘ê°„ ìš”ì•½(150í† í°)ì„ ê±°ì¹˜ì§€ ì•Šê³  ë°”ë¡œ 'ìµœì¢… ëª©í‘œ ê¸¸ì´(final_max)'ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
        if not intermediate_summaries:
            print(" (2/3) ë‹¨ì¼ ì²­í¬ ìš”ì•½ ì‹¤í–‰ (Reduce ìƒëµ)...")

            # â­ï¸ ì•ˆì „ ì¥ì¹˜: ì›ë¬¸ì´ ë„ˆë¬´ ì§§ì€ë° min_lengthê°€ í¬ë©´ í™˜ê°(ë°˜ë³µ) ë°œìƒí•˜ë¯€ë¡œ ì¡°ì ˆ
            input_len = len(kobart_tokenizer.tokenize(chunk_text))
            safe_min = min(final_min, input_len)  # ì›ë¬¸ë³´ë‹¤ ê¸¸ê²Œ ìš”ì•½í•˜ë¼ê³  ê°•ì œí•˜ì§€ ì•ŠìŒ

            # ì—¬ê¸°ì„œ ë°”ë¡œ ìµœì¢… ê²°ê³¼ ìƒì„±
            final_summary_text = _summarize_internal(chunk_text, max_gen_len=final_max, min_gen_len=safe_min)

            # í¬ë§·íŒ… í›„ ë°”ë¡œ ë¦¬í„´
            final_summary_formatted = final_summary_text.replace(". ", ".\n")
            print("âœ… ìš”ì•½ ì‘ì—… ì™„ë£Œ.")
            return final_summary_formatted

        else:
            # ì´ì „ ì²­í¬ë“¤ì´ ìˆë‹¤ë©´ ì´ê²ƒë„ ê·¸ëƒ¥ ì¤‘ê°„ ìš”ì•½ì˜ í•˜ë‚˜ì¼ ë¿ì„
            chunk_summary = _summarize_internal(chunk_text, max_gen_len=150, min_gen_len=30)
            intermediate_summaries.append(chunk_summary)

    if not intermediate_summaries:
        return "[ìš”ì•½ ìƒì„± ì‹¤íŒ¨]"

    print(f" (2/3) {len(intermediate_summaries)}ê°œ ì¤‘ê°„ ìš”ì•½ ìƒì„± ì™„ë£Œ.")

    # â­ï¸ 4. Reduce ë‹¨ê³„: ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì„ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summary_text = "\n".join(intermediate_summaries)

    print(" (3/3) ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì„ í•©ì³ ìµœì¢… ìš”ì•½ ì¤‘...")
    # Reduce ë‹¨ê³„ì—ì„œë„ ì•ˆì „ ì¥ì¹˜ ì ìš©
    input_len = len(kobart_tokenizer.tokenize(combined_summary_text))
    safe_min = min(final_min, input_len)

    final_summary_text = _summarize_internal(combined_summary_text, max_gen_len=final_max, min_gen_len=safe_min)

    # â­ï¸ 5. ìµœì¢… í¬ë§·íŒ…
    final_summary_formatted = final_summary_text.replace(". ", ".\n")
    print("âœ… ìš”ì•½ ì‘ì—… ì™„ë£Œ.")

    return final_summary_formatted


def generate_summary_thread(latest_data):
    """(êµ¬ë²„ì „ í˜¸í™˜ìš©)"""
    pass