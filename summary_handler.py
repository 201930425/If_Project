import torch
import traceback
import threading
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from db_handler import fetch_data_from_db
from config import KOBART_MODEL_NAME

# --- KoBART ëª¨ë¸ ìƒíƒœ ë³€ìˆ˜ ---
kobart_model = None
kobart_tokenizer = None
kobart_loading = False
latest_summary = "[ìš”ì•½ì€ 'ìš”ì•½ ë³´ê¸°'ë¥¼ ëˆ„ë¥´ì„¸ìš”]"


# -----------------------------

def load_kobart_model():
    """KoBART ëª¨ë¸ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    global kobart_model, kobart_tokenizer, kobart_loading
    if kobart_model is None and not kobart_loading:
        kobart_loading = True
        print("ğŸ”„ KoBART ëª¨ë¸ ë¡œë“œ ì¤‘... (ìµœì´ˆ 1íšŒ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        try:
            kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained(KOBART_MODEL_NAME)
            kobart_model = BartForConditionalGeneration.from_pretrained(
                KOBART_MODEL_NAME,
                ignore_mismatched_sizes=True
            )
            print("âœ… KoBART ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"âš ï¸ KoBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            kobart_model = None
            kobart_tokenizer = None
        finally:
            kobart_loading = False


def summarize_text(text, max_len=256):
    """KoBART ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
    global kobart_tokenizer, kobart_model
    if not text.strip():
        return "[ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤]"
    if kobart_model is None or kobart_tokenizer is None:
        return "[KoBART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]"

    try:
        inputs = kobart_tokenizer(
            text,
            return_tensors="pt",
            max_length=1024,
            truncation=True,
            padding="max_length"
        )
        summary_ids = kobart_model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            num_beams=4,
            max_length=max_len,
            early_stopping=True,
            no_repeat_ngram_size=2
        )
        summary = kobart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        return summary
    except Exception as e:
        print(f"âš ï¸ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return "[ìš”ì•½ ìƒì„± ì‹¤íŒ¨]"


def generate_summary_thread(latest_data):
    """í˜„ì¬ ì„¸ì…˜ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ìš”ì•½í•˜ê³  ì „ì—­ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    global latest_summary
    print("ğŸ”„ ìš”ì•½ ìƒì„± ì‹œì‘...")
    session_id = latest_data.get("session_id")  # .get()ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
    if not session_id:
        latest_summary = "[ì„¸ì…˜ì´ ì‹œì‘ë˜ì§€ ì•Šì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤]"
        return

    full_text = fetch_data_from_db(session_id)
    if full_text:
        latest_summary = summarize_text(full_text)
        print(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ (ì„¸ì…˜: {session_id})")
    elif not full_text:
        latest_summary = "[DBì— ìš”ì•½í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤]"

