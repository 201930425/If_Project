import torch
import traceback
import threading
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
from db_handler import fetch_data_from_db
from config import KOBART_MODEL_NAME
import math

# --- KoBART ëª¨ë¸ ìƒíƒœ ë³€ìˆ˜ ---
kobart_model = None
kobart_tokenizer = None
kobart_loading = False
latest_summary = "[ìš”ì•½ì€ 'ìš”ì•½ ë³´ê¸°'ë¥¼ ëˆ„ë¥´ì„¸ìš”]"
DEVICE = "cpu"  # â­ï¸ KoBARTëŠ” CPUë¡œ ì‹¤í–‰ (VRAM ë¶€ì¡±)


# -----------------------------

def load_kobart_model():
    """KoBART ëª¨ë¸ì„ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    global kobart_model, kobart_tokenizer, kobart_loading, latest_summary, DEVICE
    if kobart_model is None and not kobart_loading:
        kobart_loading = True
        latest_summary = "[KoBART ëª¨ë¸ ë¡œë“œ ì¤‘...]"  # ìƒíƒœ ì—…ë°ì´íŠ¸

        # â­ï¸ (VRAM 2GBë¡œëŠ” GPU ê°€ì† ì‹¤íŒ¨)
        DEVICE = "cpu"
        print(f"ğŸ”„ KoBART ëª¨ë¸ ë¡œë“œ ì¤‘... (Device: {DEVICE})")

        try:
            kobart_tokenizer = PreTrainedTokenizerFast.from_pretrained(
                KOBART_MODEL_NAME,
                ignore_mismatched_sizes=True
            )
            kobart_model = BartForConditionalGeneration.from_pretrained(
                KOBART_MODEL_NAME,
                ignore_mismatched_sizes=True
            )
            kobart_model.to(DEVICE)  # â­ï¸ CPUë¡œ ì„¤ì •

            print(f"âœ… KoBART ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. (Device: {DEVICE})")
            latest_summary = "[ëª¨ë¸ ë¡œë“œ ì™„ë£Œ. ìš”ì•½ ë²„íŠ¼ì„ ë‹¤ì‹œ ëˆŒëŸ¬ì£¼ì„¸ìš”]"
            return True
        except Exception as e:
            print(f"âš ï¸ KoBART ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            kobart_model = None
            kobart_tokenizer = None
            latest_summary = "[ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”]"
            return False
        finally:
            kobart_loading = False


# â­ï¸ [ì‹ ê·œ] í—¬í¼ í•¨ìˆ˜: ì‹¤ì œ ìš”ì•½ ì‹¤í–‰ê¸°
def _summarize_internal(text_chunk):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì¡°ê°(chunk)ì„ ìš”ì•½í•©ë‹ˆë‹¤."""
    global kobart_tokenizer, kobart_model, DEVICE

    try:
        # <s>, </s> íƒœê·¸ ì¶”ê°€
        text_with_tags = '<s>' + text_chunk + '</s>'

        inputs = kobart_tokenizer(
            text_with_tags,
            return_tensors="pt",
            max_length=1024,  # â­ï¸ ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´
            truncation=True,
            padding="max_length"
        )

        # â­ï¸ ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ë™ì¼í•œ ì¥ì¹˜ë¡œ ì´ë™
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}

        summary_ids = kobart_model.generate(
            inputs['input_ids'],
            attention_mask=inputs['attention_mask'],
            num_beams=4,
            max_length=150,  # ì¤‘ê°„ ìš”ì•½ ìµœëŒ€ ê¸¸ì´
            min_length=30,  # ì¤‘ê°„ ìš”ì•½ ìµœì†Œ ê¸¸ì´
            early_stopping=True,
            no_repeat_ngram_size=2
        )

        summary_raw = kobart_tokenizer.decode(summary_ids[0])
        summary_cleaned = summary_raw.replace('<s>', '').replace('</s>', '').replace('<usr>', '').strip()

        return summary_cleaned

    except Exception as e:
        print(f"âš ï¸ ìš”ì•½(ë‚´ë¶€) ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return "[ìš”ì•½ ì¡°ê° ìƒì„± ì‹¤íŒ¨]"


# â­ï¸ [ìˆ˜ì •] Map-Reduce ë¡œì§ì´ ì ìš©ëœ ë©”ì¸ ìš”ì•½ í•¨ìˆ˜
def summarize_text(text, max_len=256):  # max_lenì€ ìµœì¢… ìš”ì•½ë³¸ ê¸°ì¤€
    """
    KoBART ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤.
    1024 í† í°ì´ ë„˜ëŠ” ê¸´ í…ìŠ¤íŠ¸ëŠ” Map-Reduce ë°©ì‹ìœ¼ë¡œ ìë™ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    global kobart_tokenizer, kobart_model
    if not text.strip():
        return "[ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤]"
    if kobart_model is None or kobart_tokenizer is None:
        load_kobart_model()  # â­ï¸ ëª¨ë¸ì´ ì—†ìœ¼ë©´ ë¡œë“œ ì‹œë„
        if kobart_model is None:
            return "[KoBART ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤]"

    print("ğŸ”„ ìš”ì•½ ì‘ì—… ì‹œì‘...")

    # â­ï¸ 1. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥(ì¤„ë°”ê¿ˆ) ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬
    # (db_handler.pyê°€ \nìœ¼ë¡œ í•©ì³ì£¼ê¸°ë¡œ í•¨)
    sentences = [s.strip() for s in text.split('\n') if s.strip()]
    if not sentences:
        return "[ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤]"

    # â­ï¸ 2. Map ë‹¨ê³„: ë¬¸ì¥ë“¤ì„ 1024 í† í° ì²­í¬ë¡œ ë¬¶ê¸°
    max_chunk_tokens = 1000  # 1024ì˜ ì•ˆì „ ë§ˆì§„
    current_chunk_sentences = []
    current_chunk_tokens = 0
    intermediate_summaries = []

    print(f" (1/3) ì´ {len(sentences)}ê°œ ë¬¸ì¥ ì²­í¬í™” ì‹œì‘...")

    for sentence in sentences:
        # í˜„ì¬ ë¬¸ì¥ì˜ í† í° ìˆ˜ ê³„ì‚°
        sentence_tokens = len(kobart_tokenizer.tokenize(sentence))

        if current_chunk_tokens + sentence_tokens > max_chunk_tokens:
            # â­ï¸ í† í° í•œë„ ì´ˆê³¼: í˜„ì¬ê¹Œì§€ì˜ ì²­í¬ë¥¼ ìš”ì•½
            if current_chunk_sentences:
                chunk_text = " ".join(current_chunk_sentences)
                print(f"  ... ì²­í¬ ìš”ì•½ ì¤‘ (í† í° ì•½ {current_chunk_tokens}ê°œ)")
                chunk_summary = _summarize_internal(chunk_text)
                intermediate_summaries.append(chunk_summary)

            # ìƒˆ ì²­í¬ ì‹œì‘
            current_chunk_sentences = [sentence]
            current_chunk_tokens = sentence_tokens
        else:
            # â­ï¸ í† í° í•œë„ ë¯¸ë§Œ: í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€
            current_chunk_sentences.append(sentence)
            current_chunk_tokens += sentence_tokens

    # â­ï¸ ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ìš”ì•½
    if current_chunk_sentences:
        print(f"  ... ë§ˆì§€ë§‰ ì²­í¬ ìš”ì•½ ì¤‘ (í† í° ì•½ {current_chunk_tokens}ê°œ)")
        chunk_text = " ".join(current_chunk_sentences)
        chunk_summary = _summarize_internal(chunk_text)
        intermediate_summaries.append(chunk_summary)

    if not intermediate_summaries:
        return "[ìš”ì•½ ìƒì„± ì‹¤íŒ¨]"

    print(f" (2/3) {len(intermediate_summaries)}ê°œ ì¤‘ê°„ ìš”ì•½ ìƒì„± ì™„ë£Œ.")

    # â­ï¸ 3. Reduce ë‹¨ê³„: ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì„ í•©ì³ì„œ ìµœì¢… ìš”ì•½
    combined_summary_text = "\n".join(intermediate_summaries)

    # â­ï¸ ë§Œì•½ ì¤‘ê°„ ìš”ì•½ì´ 1ê°œ ë¿ì´ë©´ (í…ìŠ¤íŠ¸ê°€ 1024 í† í° ë¯¸ë§Œì´ì—ˆìœ¼ë©´)
    if len(intermediate_summaries) == 1:
        final_summary_text = intermediate_summaries[0]
    else:
        # â­ï¸ ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì˜ í•©ì´ 1024 í† í°ì„ ë„˜ìœ¼ë©´, ìµœì¢… ìš”ì•½ë„ ì˜ë¦´ ìˆ˜ ìˆì§€ë§Œ
        # (ì´ ê²½ìš° ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•˜ë‚˜, CPU ë¶€ë‹´ìœ¼ë¡œ 1íšŒë¡œ ì œí•œ)
        print(" (3/3) ì¤‘ê°„ ìš”ì•½ë³¸ë“¤ì„ í•©ì³ ìµœì¢… ìš”ì•½ ì¤‘...")
        final_summary_text = _summarize_internal(combined_summary_text)

    # â­ï¸ 4. ìµœì¢… í¬ë§·íŒ… (ì¤„ë°”ê¿ˆ ì¶”ê°€)
    final_summary_formatted = final_summary_text.replace(". ", ".\n")
    print("âœ… ìš”ì•½ ì‘ì—… ì™„ë£Œ.")

    return final_summary_formatted


def generate_summary_thread(latest_data):
    """
    (ê¹ƒ ì˜¤ë¦¬ì§€ë„ ë²„ì „)
    *í˜„ì¬* ì„¸ì…˜ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ìš”ì•½í•˜ê³  ì „ì—­ ë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    global latest_summary
    print("ğŸ”„ ìš”ì•½ ìƒì„± ì‹œì‘...")
    latest_summary = "[ìš”ì•½ ìƒì„± ì¤‘...]"  # ìƒíƒœ ì—…ë°ì´íŠ¸

    session_id = latest_data.get("session_id")  # .get()ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì ‘ê·¼
    if not session_id:
        latest_summary = "[ìš”ì•½í•  ì„¸ì…˜ IDê°€ ì—†ìŠµë‹ˆë‹¤]"
        return

    full_text = fetch_data_from_db(session_id)  # *í˜„ì¬* ì„¸ì…˜ IDë¡œ ì¡°íšŒ
    if full_text:
        latest_summary = summarize_text(full_text)
        print(f"âœ… ìš”ì•½ ìƒì„± ì™„ë£Œ (ì„¸ì…˜: {session_id})")
    elif not full_text:
        latest_summary = "[DBì— ìš”ì•½í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤]"