import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel
import queue
import time
from deep_translator import GoogleTranslator

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from config import (
    MODEL_TYPE, LANGUAGE, BLOCK_DURATION, TARGET_LANG,
    VOLUME_THRESHOLD, BEAM_SIZE
)
from db_handler import insert_transcript

# (OBS ê´€ë ¨ 'utils' ì„í¬íŠ¸ ì œê±°)

# --- Whisper ëª¨ë¸ ë° ì˜¤ë””ì˜¤ í ---
print(f"ğŸ§ Whisper ëª¨ë¸ ({MODEL_TYPE}) ë¡œë“œ ì¤‘...")
model = WhisperModel(MODEL_TYPE, device="cpu", compute_type="int8")
audio_q = queue.Queue()


# --------------------------------

def audio_callback(indata, frames, time_, status):
    """ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ íì— ë„£ìŠµë‹ˆë‹¤."""
    if status:
        print(status)
    audio_q.put(indata.copy())


def translate_text_local(text, target_lang=TARGET_LANG):
    """í…ìŠ¤íŠ¸ë¥¼ ë²ˆì—­í•©ë‹ˆë‹¤."""
    if not text.strip():
        return "[ë¹ˆ ë¬¸ìì—´]"
    try:
        translated = GoogleTranslator(source='auto', target=target_lang).translate(text)
        return translated
    except Exception as e:
        print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return "[ë²ˆì—­ ì‹¤íŒ¨]"


def is_speech(buffer, threshold=VOLUME_THRESHOLD):
    """ìµœì†Œ ë³¼ë¥¨ì„ ì²´í¬í•©ë‹ˆë‹¤."""
    rms = np.sqrt(np.mean(buffer ** 2))
    return rms > threshold


def main_audio_loop(session_id, latest_data):
    """
    ë©”ì¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ìŠ¤ë ˆë“œ í•¨ìˆ˜.
    latest_data ë”•ì…”ë„ˆë¦¬ë¥¼ ì§ì ‘ ìˆ˜ì •í•˜ì—¬ app.pyì™€ í†µì‹ í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ—‚ï¸ ì„¸ì…˜ ì‹œì‘: {session_id}")
    latest_data["session_id"] = session_id

    with sd.InputStream(samplerate=16000, channels=1, callback=audio_callback):
        print("ğŸ¤ [ìŠ¤ë ˆë“œ] ìŒì„± ì¸ì‹ + ë²ˆì—­ + DB ì €ì¥ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")
        buffer = np.zeros((0,), dtype=np.float32)
        last_text = ""

        while True:
            try:
                # íì—ì„œ ë°ì´í„° ê°€ì ¸ì™€ ë²„í¼ì— ëˆ„ì 
                while not audio_q.empty():
                    block = audio_q.get()
                    buffer = np.concatenate((buffer, block.flatten()))

                # ë²„í¼ê°€ ìµœì†Œ ì²˜ë¦¬ ë‹¨ìœ„(BLOCK_DURATION)ë³´ë‹¤ ì§§ìœ¼ë©´ ëŒ€ê¸°
                if len(buffer) < 16000 * BLOCK_DURATION:
                    time.sleep(0.1)
                    continue

                # ì²˜ë¦¬í•  ì„¸ê·¸ë¨¼íŠ¸ ì¤€ë¹„ ë° ë²„í¼ ë¹„ìš°ê¸° (ë”œë ˆì´ ë°©ì§€)
                segment_to_process = buffer
                buffer = np.zeros((0,), dtype=np.float32)

                if is_speech(segment_to_process):

                    segments, _ = model.transcribe(
                        segment_to_process.flatten(),
                        language=LANGUAGE,
                        beam_size=BEAM_SIZE
                    )

                    combined_text = " ".join(seg.text.strip() for seg in segments if seg.text.strip())

                    if combined_text and combined_text != last_text:
                        last_text = combined_text
                        print(f"ğŸ¤ ì¸ì‹: {combined_text}")
                        translated = translate_text_local(combined_text)
                        print(f"ğŸŒ ë²ˆì—­: {translated}")

                        # --- OBS íŒŒì¼ ì—…ë°ì´íŠ¸ ì½”ë“œ ì œê±°ë¨ ---

                        # DB ì—…ë°ì´íŠ¸
                        insert_transcript(session_id, combined_text, translated)

                        # app.pyì™€ í†µì‹  (ë©”ì¸ ìŠ¤ë ˆë“œìš©)
                        latest_data["original"] = combined_text
                        latest_data["translated"] = translated

            except KeyboardInterrupt:
                print("ğŸ›‘ [ìŠ¤ë ˆë“œ] ìŒì„± ì¸ì‹ ì¢…ë£Œ.")
                break
            except Exception as e:
                print(f"ì˜¤ë””ì˜¤ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(1)

