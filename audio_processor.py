import sounddevice as sd
import numpy as np
import queue
import time
import webrtcvad
from faster_whisper import WhisperModel
from deep_translator import GoogleTranslator
from datetime import datetime, timezone, timedelta
import traceback
import noisereduce as nr
import asyncio
import edge_tts  # ğŸ”Š ì¶”ê°€ë¨ â€” Edge TTS
import tempfile
import os
import soundfile as sf


import config
from db_handler import insert_transcript
from config import (
    MODEL_TYPE, LANGUAGE, TARGET_LANG,
    BEAM_SIZE, INPUT_DEVICE_INDEX, FRAME_SIZE,
    VAD_MODE, FRAME_DURATION_MS, RATE, CHUNK_DURATION_SEC, CHUNK_SIZE
)

print(f"ğŸ§ Whisper ëª¨ë¸({MODEL_TYPE}) ë¡œë“œ ì¤‘...")
model = WhisperModel(MODEL_TYPE, device="cpu", compute_type="int8")
vad = webrtcvad.Vad(VAD_MODE)
audio_q = queue.Queue()

# --- ë²ˆì—­ ---
def translate_text_local(text, target_lang=TARGET_LANG):
    if not text or not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target=target_lang).translate(text)
    except Exception as e:
        print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return ""

# --- ğŸ”Š Edge-TTS ë¹„ë™ê¸° ìŒì„± ì¶œë ¥ (ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥) ---
async def speak_text_async(text, target_lang=TARGET_LANG):
    """ë¹„ë™ê¸° ìŒì„± ì¶œë ¥"""
    if not text.strip():
        return

    try:
        # ì–¸ì–´ë³„ ìŒì„± ì„ íƒ
        voice_map = {
            "ko": "ko-KR-SunHiNeural",
            "en": "en-US-JennyNeural",
            "ja": "ja-JP-NanamiNeural",
            "zh-CN": "zh-CN-XiaoxiaoNeural",
            "de": "de-DE-KatjaNeural",
            "fr": "fr-FR-DeniseNeural"
        }
        voice = voice_map.get(target_lang, "en-US-JennyNeural")

        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as tmp:
            output_path = tmp.name

        communicate = edge_tts.Communicate(text, voice=voice)
        await communicate.save(output_path)

        # ì¬ìƒì„ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ìˆ˜í–‰ â†’ Whisper ì¸ì‹ ì¤‘ì—ë„ ì¬ìƒë¨
        def play_audio_file():
            try:
                data, samplerate = sf.read(output_path)
                sd.play(data, samplerate)
                sd.wait()
            except Exception as e:
                print(f"âš ï¸ ìŒì„± ì¬ìƒ ì˜¤ë¥˜: {e}")
            finally:
                try:
                    os.remove(output_path)
                except:
                    pass

        # ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, play_audio_file)

    except Exception as e:
        print(f"âš ï¸ ìŒì„± ì¶œë ¥ ì‹¤íŒ¨: {e}")


def speak_text(text, target_lang=TARGET_LANG):
    """ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œë„ ì•ˆì „í•˜ê²Œ Edge-TTS ì‹¤í–‰"""
    if not text.strip():
        return

    try:
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = None

        if loop and loop.is_running():
            # ì´ë¯¸ asyncio ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë¼ë©´ ë¹„ë™ê¸°ë¡œ íƒœìŠ¤í¬ ìƒì„±
            asyncio.create_task(speak_text_async(text, target_lang))
        else:
            # í˜„ì¬ ìŠ¤ë ˆë“œì— ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            new_loop = asyncio.new_event_loop()
            asyncio.set_event_loop(new_loop)
            new_loop.run_until_complete(speak_text_async(text, target_lang))
            new_loop.close()

    except Exception as e:
        print(f"âš ï¸ speak_text ì‹¤í–‰ ì˜¤ë¥˜: {e}")



# --- ë¬¸ì¥ ì™„ì„± ê°ì§€ (êµ¬ë‘ì  + ë¬´ìŒ ê¸°ë°˜) ---
def is_sentence_complete(text):
    """ë¬¸ì¥ì´ ëë‚¬ëŠ”ì§€ íŒë³„"""
    if not text.strip():
        return False
    text = text.strip()
    return text.endswith((".", "!", "?", "ìš”", "ë‹¤", "ì£ ", "ë„¤", "ìŠµë‹ˆë‹¤"))

# --- ì˜¤ë””ì˜¤ ì½œë°± ---
def audio_callback(indata, frames, time_, status):
    if status:
        print(f"[Audio status] {status}")
    try:
        audio_q.put(indata.copy())
    except Exception:
        traceback.print_exc()

# --- ìŒì„± ê°ì§€ ---
def is_speech_chunk(data_chunk, rate=RATE, frame_ms=30):
    """RMS + VAD ê¸°ë°˜ ìŒì„± ê°ì§€"""
    frame_length = int(rate * frame_ms / 1000)
    bytes_data = data_chunk.tobytes()
    speech_frames = 0
    frame_count = 0

    for i in range(0, len(bytes_data), frame_length * 2):
        frame = bytes_data[i:i + frame_length * 2]
        if len(frame) < frame_length * 2:
            break
        frame_count += 1
        try:
            if vad.is_speech(frame, rate):
                speech_frames += 1
        except webrtcvad.Error:
            continue

    return speech_frames > 0

# --- ë©”ì¸ ë£¨í”„ ---
def main_audio_streaming(session_id, socketio, stop_event=None):
    print(f"ğŸ—‚ï¸ ì„¸ì…˜ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ): {session_id}")

    buffer = np.zeros((0, 1), dtype=np.int16)
    sentence_buffer = ""
    previous_text = ""
    last_emit_time = time.time()
    silence_counter = 0

    try:
        with sd.InputStream(
            device=INPUT_DEVICE_INDEX,
            samplerate=RATE,
            blocksize=FRAME_SIZE,
            dtype='int16',
            channels=1,
            callback=audio_callback
        ):
            print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘...")

            while True:
                if stop_event is not None and stop_event.is_set():
                    print("ğŸ›‘ stop_event ìˆ˜ì‹ : ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                if not audio_q.empty():
                    block = audio_q.get()
                    buffer = np.concatenate((buffer, block), axis=0)

                    if len(buffer) >= CHUNK_SIZE:
                        data_chunk = buffer[:CHUNK_SIZE]
                        buffer = buffer[CHUNK_SIZE:]

                        try:
                            # ğŸ”‰ ìŒì„± ê°ì§€
                            if not is_speech_chunk(data_chunk, RATE):
                                silence_counter += 1
                                if silence_counter >= 2 and sentence_buffer.strip():
                                    # âœ… 1.5ì´ˆ ì´ìƒ ë¬´ìŒ â†’ ë¬¸ì¥ ì™„ë£Œë¡œ ê°„ì£¼
                                    kst = timezone(timedelta(hours=9))
                                    now_time = datetime.now(kst).strftime("%H:%M:%S")

                                    translated = translate_text_local(sentence_buffer)
                                    print(f"âœ… ì™„ì„± ë¬¸ì¥: {sentence_buffer}")
                                    print(f"ğŸŒ ë²ˆì—­ ê²°ê³¼: {translated}\n")

                                    # ğŸ”Š ë²ˆì—­ëœ ë¬¸ì¥ ìŒì„± ì¶œë ¥
                                    speak_text(translated, TARGET_LANG)

                                    socketio.emit('partial_translation', {
                                        'original': sentence_buffer.strip(),
                                        'translated': translated,
                                        'time': now_time,
                                        'session_id': session_id
                                    })

                                    insert_transcript(session_id, sentence_buffer.strip(), translated)
                                    sentence_buffer = ""
                                    previous_text = ""
                                    silence_counter = 0
                                continue
                            else:
                                silence_counter = 0

                                # ğŸ”‰ ë…¸ì´ì¦ˆ ì œê±°
                                reduced = nr.reduce_noise(y=data_chunk.flatten(), sr=RATE)

                                # â­ï¸ [ìˆ˜ì •] 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì˜¤ë¥˜(RuntimeWarning) ë°©ì§€
                                max_val = np.max(np.abs(reduced))

                                if max_val > 0:
                                    normalized_audio = reduced / max_val
                                else:
                                    normalized_audio = reduced

                                reduced_int16 = np.int16(normalized_audio * 32767)
                                audio_float32 = reduced_int16.astype(np.float32) / 32768.0

                            # ğŸ§  Whisper ì¸ì‹
                            segments, _ = model.transcribe(
                                audio_float32,
                                language=config.LANGUAGE,
                                beam_size=BEAM_SIZE,
                                vad_filter=True,
                                no_speech_threshold=0.6,
                                log_prob_threshold=-1.0,
                                condition_on_previous_text=False
                            )
                            partial_text = " ".join(seg.text.strip() for seg in segments if seg.text.strip())

                            if partial_text and partial_text != previous_text:
                                new_part = partial_text.replace(previous_text, "").strip()
                                if new_part:
                                    sentence_buffer += " " + new_part
                                    previous_text = partial_text
                                    print(f"ğŸ§© ë¶€ë¶„ ì¸ì‹ ëˆ„ì : {new_part}")

                                # ì¢…ê²°ì–´ë¯¸ ê¸°ë°˜ ë¬¸ì¥ ì™„ì„± ê°ì§€
                                if is_sentence_complete(sentence_buffer):
                                    kst = timezone(timedelta(hours=9))
                                    now_time = datetime.now(kst).strftime("%H:%M:%S")

                                    translated = translate_text_local(sentence_buffer)
                                    print(f"âœ… ì™„ì„± ë¬¸ì¥: {sentence_buffer}")
                                    print(f"ğŸŒ ë²ˆì—­ ê²°ê³¼: {translated}\n")

                                    # ğŸ”Š ë²ˆì—­ëœ ë¬¸ì¥ ìŒì„± ì¶œë ¥
                                    speak_text(translated, TARGET_LANG)

                                    socketio.emit('partial_translation', {
                                        'original': sentence_buffer.strip(),
                                        'translated': translated,
                                        'time': now_time,
                                        'session_id': session_id
                                    })

                                    insert_transcript(session_id, sentence_buffer.strip(), translated)
                                    sentence_buffer = ""
                                    previous_text = ""

                        except Exception as e:
                            print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            traceback.print_exc()
                else:
                    time.sleep(0.01)

    except sd.PortAudioError as e:
        print("âŒ ì˜¤ë””ì˜¤ ì¥ì¹˜ ì˜¤ë¥˜:", e)
    except Exception as e:
        print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜:", e)
        traceback.print_exc()
