# # â­ï¸ [ì‹ ê·œ] CUDA DLL ê²½ë¡œë¥¼ ìŠ¤í¬ë¦½íŠ¸ ìµœìƒë‹¨ì— ì§ì ‘ ì¶”ê°€ #gpuì‚¬ìš©ì‹œ
# import os
#
# # 1. CUDA Toolkit ê²½ë¡œ (ê¸°ì¡´)
# cuda_toolkit_path = r"C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1\bin"
# # â­ï¸ 2. cuDNN ê²½ë¡œ (ìƒˆë¡œ ì°¾ì€ ì •í™•í•œ ê²½ë¡œ)
# cudnn_path = r"C:\Program Files\NVIDIA\CUDNN\v9.15\bin\12.9"
#
# # â­ï¸ [ìˆ˜ì •] 2ê°œì˜ ê²½ë¡œë¥¼ ëª¨ë‘ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
# paths_to_add = [cuda_toolkit_path, cudnn_path]
#
# for path in paths_to_add:
#     # â­ï¸ 3. os.environ["PATH"]ì— ìˆ˜ë™ ì¶”ê°€ (MINGW64 í˜¸í™˜ì„±)
#     try:
#         if path and os.path.exists(path) and path not in os.environ.get("PATH", ""):
#             print(f"âœ… (ìµœìƒë‹¨) os.environ['PATH']ì— ê²½ë¡œ ì¶”ê°€: {path}")
#             os.environ["PATH"] = path + os.pathsep + os.environ.get("PATH", "")
#         elif not os.path.exists(path):
#              print(f"âš ï¸ (ìµœìƒë‹¨) ê²½ê³ : ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
#     except Exception as e:
#         print(f"âš ï¸ (ìµœìƒë‹¨) PATH í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
#
#     # â­ï¸ 4. (ê¸°ì¡´) os.add_dll_directory ì‚¬ìš© (Python 3.8+ ê¶Œì¥ ë°©ì‹)
#     try:
#         if path and os.path.exists(path):
#             print(f"âœ… (ìµœìƒë‹¨) os.add_dll_directoryë¡œ ê²½ë¡œ ì¶”ê°€: {path}")
#             os.add_dll_directory(path)
#     except Exception as e:
#         print(f"âš ï¸ (ìµœìƒë‹¨) DLL ê²½ë¡œ ì¶”ê°€ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
# # â­ï¸ [ì‹ ê·œ] ì—¬ê¸°ê¹Œì§€ ìˆ˜ì • ---
import os
import sounddevice as sd
import numpy as np
import queue
import time
# import webrtcvad # âŒ (ì œê±°)
import torch  # â­ï¸ [ì¶”ê°€] Silero VADì— í•„ìš”
from faster_whisper import WhisperModel
from deep_translator import GoogleTranslator
from datetime import datetime, timezone, timedelta
import traceback
import noisereduce as nr
import wave

import config
from db_handler import insert_transcript
from config import (
    MODEL_TYPE, LANGUAGE, TARGET_LANG,
    BEAM_SIZE, INPUT_DEVICE_INDEX, FRAME_SIZE,
    VAD_THRESHOLD, RATE, CHUNK_DURATION_SEC, CHUNK_SIZE
)

print(f"ğŸ§ Whisper ëª¨ë¸({MODEL_TYPE}) ë¡œë“œ ì¤‘...")
# â­ï¸ [ìˆ˜ì •] float16 -> int8_float16 (GTX 1050 í˜¸í™˜ ëª¨ë“œ)
# model = WhisperModel(MODEL_TYPE, device="cuda", compute_type="int8") #gpuì‚¬ìš©ì‹œ
# print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: CUDA/GPU)") #gpuì‚¬ìš©ì‹œ
model = WhisperModel(MODEL_TYPE, device="cpu", compute_type="int8")
print("âœ… Whisper ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: CPU)")


# â­ï¸ Silero VAD ëª¨ë¸ ë¡œë“œ
print("ğŸ§ Silero VAD ëª¨ë¸ ë¡œë“œ ì¤‘... (torch í•„ìš”)")
try:
    vad_model, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad',
                                  model='silero_vad',
                                  force_reload=False,
                                  onnx=True)
    # â­ï¸ [ì œê±°] ì´ ë¼ì¸ì„ ì‚­ì œí•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•˜ì„¸ìš”.
    # vad_model.to("cuda")
    print("âœ… Silero VAD (ONNX) ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (Device: CPU).") # â­ï¸ ë¡œê·¸ ìˆ˜ì •
except Exception as e:
    print(f"âš ï¸ Silero VAD ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("torch, torchaudioê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€, ì¸í„°ë„· ì—°ê²°ì´ ë˜ì–´ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    vad_model = None

# âŒ (ì œê±°) vad = webrtcvad.Vad(VAD_MODE)
audio_q = queue.Queue()


# --- ë²ˆì—­ ---
def translate_text_local(text, target_lang=TARGET_LANG):
    if not text or not text.strip():
        return ""

    # â­ í•œêµ­ì–´ íƒ­ â†’ ë²ˆì—­ í•„ìš” ì—†ìŒ
    if target_lang == config.LANGUAGE:
        return ""

    try:
        return GoogleTranslator(source='auto', target=target_lang).translate(text)
    except Exception as e:
        print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return ""


# --- ë¬¸ì¥ ì™„ì„± ê°ì§€ (êµ¬ë‘ì  + ë¬´ìŒ ê¸°ë°˜) ---
def is_sentence_complete(text):
    """ë¬¸ì¥ì´ ëë‚¬ëŠ”ì§€ íŒë³„"""
    if not text.strip():
        return False
    text = text.strip()
    endings = (
        ".", "!", "?",  # ì˜ì–´
        "ìš”", "ë‹¤", "ì£ ", "ë„¤", "ìŠµë‹ˆë‹¤",  # í•œêµ­ì–´
        "ã€‚", "ã§ã™", "ã¾ã™", "ã­", "ã‹"  # â­ï¸ ì¼ë³¸ì–´ ì¶”ê°€
    )
    return text.endswith(endings)


# â­ï¸ [ì‹ ê·œ] Silero VADìš© í—¬í¼ í•¨ìˆ˜ (ë²„ê·¸ ìˆ˜ì •)
def is_chunk_speech(data_chunk, vad_model, rate=RATE, frame_size=FRAME_SIZE, threshold=VAD_THRESHOLD):
    """
    ê¸´ ì˜¤ë””ì˜¤ ì²­í¬(data_chunk)ë¥¼ VAD_FRAME_SIZE(512) ìƒ˜í”Œ í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´
    í•˜ë‚˜ë¼ë„ ìŒì„±ìœ¼ë¡œ ê°ì§€ë˜ë©´ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not vad_model:
        print("âš ï¸ VAD ëª¨ë¸ì´ ì—†ì–´ ìŒì„±ìœ¼ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
        return True  # VAD ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¬´ì¡°ê±´ ìŒì„±ìœ¼ë¡œ ì²˜ë¦¬

    # data_chunkëŠ” (N, 1) í˜•íƒœì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ flatten()
    data_flat = data_chunk.flatten()

    # 512 ìƒ˜í”Œ(VAD_FRAME_SIZE) ë‹¨ìœ„ë¡œ ë°˜ë³µ
    for i in range(0, len(data_flat), frame_size):
        frame = data_flat[i: i + frame_size]

        # â­ï¸ (ì¤‘ìš”) ë§ˆì§€ë§‰ í”„ë ˆì„ì´ 512ë³´ë‹¤ ì‘ìœ¼ë©´ VADê°€ ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚´
        if len(frame) < frame_size:
            continue

        # 1. int16 numpy -> float32 tensor
        audio_float32_tensor = torch.from_numpy(frame.astype(np.float32) / 32768.0)

        # 2. VAD ëª¨ë¸ ì‹¤í–‰ (ìŒì„± í™•ë¥  ë°˜í™˜)
        try:
            # â­ï¸ .item()ì€ tensor(0.xx) -> 0.xx (float)ë¡œ ë³€í™˜
            speech_prob = vad_model(audio_float32_tensor, rate).item()

            # 3. ì„ê³„ê°’ê³¼ ë¹„êµ
            if speech_prob >= threshold:
                return True  # ìŒì„± ê°ì§€ë¨
        except Exception as e:
            # CHUNK_SIZEê°€ 512ì˜ ë°°ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš° ë“± ì˜ˆì™¸ ì²˜ë¦¬
            # print(f"VAD í”„ë ˆì„ ì²˜ë¦¬ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
            pass

            # ë£¨í”„ê°€ ëë‚  ë•Œê¹Œì§€ ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ
    return False


# --- ì˜¤ë””ì˜¤ ì½œë°± ---
def audio_callback(indata, frames, time_, status):
    if status:
        print(f"[Audio status] {status}")
    try:
        audio_q.put(indata.copy())
    except Exception:
        traceback.print_exc()


# --- âŒ (ì œê±°) 'is_speech_chunk' (webrtcvad) ---


# --- ë©”ì¸ ë£¨í”„ ---
def main_audio_streaming(session_id, socketio, stop_event=None):
    print(f"ğŸ—‚ï¸ ì„¸ì…˜ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ): {session_id}")

    # â­ï¸ [ì‹ ê·œ] .wav íŒŒì¼ ì“°ê¸° ì¤€ë¹„
    output_dir = "wav"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")

    wave_file = None
    wave_file_name = os.path.join(output_dir, f"{session_id}.wav") # wav/session_id.wav
    try:
        wave_file = wave.open(wave_file_name, 'wb')
        wave_file.setnchannels(1)  # ëª¨ë…¸ (1 ì±„ë„)
        wave_file.setsampwidth(2)  # 2ë°”ì´íŠ¸ (int16)
        wave_file.setframerate(RATE)  # 16000
        print(f"ğŸŒŠ ì˜¤ë””ì˜¤ íŒŒì¼ ë…¹ìŒ ì‹œì‘: {wave_file_name}")
    except Exception as e:
        print(f"âš ï¸ [ì˜¤ë¥˜] {wave_file_name} íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        wave_file = None  # íŒŒì¼ ì“°ê¸° ë¹„í™œì„±í™”

    buffer = np.zeros((0, 1), dtype=np.int16)
    sentence_buffer = ""
    previous_text = ""
    last_emit_time = time.time()
    silence_counter = 0

    try:
        with sd.InputStream(
                device=INPUT_DEVICE_INDEX,
                samplerate=RATE,
                blocksize=FRAME_SIZE,  # â­ï¸ config.pyì—ì„œ 512ë¡œ ë³€ê²½ë¨
                dtype='int16',
                channels=1,
                callback=audio_callback
        ):
            print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘...")

            while True:
                if stop_event is not None and stop_event.is_set():
                    print("ğŸ›‘ stop_event ìˆ˜ì‹ : ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                if not audio_q.empty():
                    block = audio_q.get()
                    # â­ï¸ [ì‹ ê·œ] 1. ì˜¤ë””ì˜¤ ì¡°ê°ì„ .wav íŒŒì¼ì— ì €ì¥
                    if wave_file:
                        try:
                            wave_file.writeframes(block.tobytes())
                        except Exception as e:
                            print(f"âš ï¸ [ì˜¤ë¥˜] {wave_file_name} íŒŒì¼ ì“°ê¸° ì¤‘ë‹¨: {e}")
                            wave_file.close()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŒŒì¼ ë‹«ê¸°
                            wave_file = None  # ë” ì´ìƒ ì“°ì§€ ì•ŠìŒ

                    buffer = np.concatenate((buffer, block), axis=0)

                    if len(buffer) >= CHUNK_SIZE:
                        data_chunk = buffer[:CHUNK_SIZE]
                        buffer = buffer[CHUNK_SIZE:]

                        try:
                            # ğŸ”‰ [ìˆ˜ì •] ìŒì„± ê°ì§€ (Silero VAD í—¬í¼ í•¨ìˆ˜ ì‚¬ìš©)
                            # â­ï¸ data_chunk(48000)ë¥¼ í—¬í¼ í•¨ìˆ˜ë¡œ ì „ë‹¬
                            if not is_chunk_speech(data_chunk, vad_model, RATE, FRAME_SIZE, VAD_THRESHOLD):
                                # (ë¬´ìŒìœ¼ë¡œ ê°„ì£¼ - ê¸°ì¡´ 'if not is_speech_chunk' ë¡œì§)
                                silence_counter += 1
                                if silence_counter >= 2 and sentence_buffer.strip():
                                    # âœ… 1.5ì´ˆ ì´ìƒ ë¬´ìŒ â†’ ë¬¸ì¥ ì™„ë£Œë¡œ ê°„ì£¼
                                    kst = timezone(timedelta(hours=9))
                                    now_time = datetime.now(kst).strftime("%H:%M:%S")

                                    translated = translate_text_local(sentence_buffer, target_lang=config.TARGET_LANG)
                                    print(f"âœ… ì™„ì„± ë¬¸ì¥: {sentence_buffer}")
                                    print(f"ğŸŒ ë²ˆì—­ ê²°ê³¼: {translated}\n")

                                    socketio.emit('partial_translation', {
                                        'original': sentence_buffer.strip(),
                                        'translated': translated,
                                        'time': now_time,
                                        'session_id': session_id
                                    })

                                    insert_transcript(session_id, sentence_buffer.strip(), translated)
                                    sentence_buffer = ""
                                    previous_text = ""
                                    silence_counter = 0

                                # â­ï¸ (ì¤‘ìš”) ì›ë³¸ ë¡œì§ê³¼ ë™ì¼í•˜ê²Œ, ìŒì„±ì´ ì•„ë‹ˆë©´ ë²ˆì—­/ì¸ì‹ ìŠ¤í‚µ
                                continue

                            else:
                                # (ìŒì„±ìœ¼ë¡œ ê°„ì£¼ - ê¸°ì¡´ 'else' ë¸”ë¡)
                                silence_counter = 0

                                # ğŸ”‰ ë…¸ì´ì¦ˆ ì œê±°
                                reduced = nr.reduce_noise(y=data_chunk.flatten(), sr=RATE)

                                # â­ï¸ [ìˆ˜ì •] 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ì˜¤ë¥˜(RuntimeWarning) ë°©ì§€
                                max_val = np.max(np.abs(reduced))

                                if max_val > 0:
                                    # ì‹ í˜¸ê°€ ìˆì„ ë•Œë§Œ ì •ê·œí™”
                                    normalized_audio = reduced / max_val
                                else:
                                    # ì™„ì „í•œ ë¬´ìŒì¸ ê²½ìš° (max_val == 0)
                                    normalized_audio = reduced  # (ì´ë¯¸ 0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´)

                                reduced_int16 = np.int16(normalized_audio * 32767)
                                audio_float32 = reduced_int16.astype(np.float32) / 32768.0

                            # ğŸ§  Whisper ì¸ì‹ (ë¬´ìŒì´ ì•„ë‹ ë•Œë§Œ ì´ìª½ìœ¼ë¡œ ë„˜ì–´ì˜´)
                            segments, _ = model.transcribe(
                                audio_float32,
                                language=config.LANGUAGE,
                                beam_size=BEAM_SIZE,
                                # --- â­ï¸ í™˜ê°(ì“°ë ˆê¸°ê°’) ì–µì œ ì˜µì…˜ ì¶”ê°€ ---
                                vad_filter=True,  # VAD í•„í„°ë¥¼ ì‚¬ìš©í•´ ìŒì„±ì´ ì—†ëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì œê±°
                                no_speech_threshold=0.4,  # ì´ ê°’ ì´í•˜ì˜ 'ìŒì„± í™•ë¥ 'ì€ ë¬´ì‹œ
                                log_prob_threshold=-1.0,  # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ì€ í† í°(ë‹¨ì–´)ì„ ì–µì œ
                                condition_on_previous_text=False  # ì´ì „ í…ìŠ¤íŠ¸ì— ëœ ì˜ì¡´í•˜ì—¬ ë°˜ë³µ í™˜ê°ì„ ì¤„ì„
                            )
                            partial_text = " ".join(seg.text.strip() for seg in segments if seg.text.strip())

                            if partial_text and partial_text != previous_text:
                                # âœ… ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                new_part = partial_text.replace(previous_text, "").strip()
                                if new_part:
                                    sentence_buffer += " " + new_part
                                    previous_text = partial_text
                                    print(f"ğŸ§© ë¶€ë¶„ ì¸ì‹ ëˆ„ì : {new_part}")

                                # ì¢…ê²°ì–´ë¯¸ ê¸°ë°˜ ë¬¸ì¥ ì™„ì„± ê°ì§€
                                if is_sentence_complete(sentence_buffer):
                                    kst = timezone(timedelta(hours=9))
                                    now_time = datetime.now(kst).strftime("%H:%M:%S")

                                    translated = translate_text_local(sentence_buffer, target_lang=config.TARGET_LANG)
                                    print(f"âœ… ì™„ì„± ë¬¸ì¥: {sentence_buffer}")
                                    print(f"ğŸŒ ë²ˆì—­ ê²°ê³¼: {translated}\n")

                                    socketio.emit('partial_translation', {
                                        'original': sentence_buffer.strip(),
                                        'translated': translated,
                                        'time': now_time,
                                        'session_id': session_id
                                    })

                                    insert_transcript(session_id, sentence_buffer.strip(), translated)
                                    sentence_buffer = ""
                                    previous_text = ""

                        except Exception as e:
                            print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            traceback.print_exc()
                else:
                    time.sleep(0.01)

    except sd.PortAudioError as e:
        print("âŒ ì˜¤ë””ì˜¤ ì¥ì¹˜ ì˜¤ë¥˜:", e)
    except Exception as e:
        print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜:", e)
        traceback.print_exc()
    finally:
        # â­ï¸ [ì‹ ê·œ] ì„¸ì…˜ì´ ëë‚˜ë©´ .wav íŒŒì¼ ë‹«ê¸°
        if wave_file:
            wave_file.close()
            print(f"ğŸŒŠ ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {wave_file_name}")