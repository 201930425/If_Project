import sounddevice as sd
import numpy as np
import queue
import time
import webrtcvad
from faster_whisper import WhisperModel
from deep_translator import GoogleTranslator
from datetime import datetime, timezone, timedelta
import traceback
import noisereduce as nr

import config
from db_handler import insert_transcript
from config import (
    MODEL_TYPE, LANGUAGE, TARGET_LANG,
    BEAM_SIZE, INPUT_DEVICE_INDEX, FRAME_SIZE,
    VAD_MODE, FRAME_DURATION_MS, RATE, CHUNK_DURATION_SEC, CHUNK_SIZE
)

print(f"ğŸ§ Whisper ëª¨ë¸({MODEL_TYPE}) ë¡œë“œ ì¤‘...")
model = WhisperModel(MODEL_TYPE, device="cpu", compute_type="int8")
vad = webrtcvad.Vad(VAD_MODE)
audio_q = queue.Queue()

# --- ë²ˆì—­ ---
def translate_text_local(text, target_lang=TARGET_LANG):
    if not text or not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target=target_lang).translate(text)
    except Exception as e:
        print(f"âš ï¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return ""

# --- ë¬¸ì¥ ì™„ì„± ê°ì§€ (êµ¬ë‘ì  + ë¬´ìŒ ê¸°ë°˜) ---
def is_sentence_complete(text):
    """ë¬¸ì¥ì´ ëë‚¬ëŠ”ì§€ íŒë³„"""
    if not text.strip():
        return False
    text = text.strip()
    return text.endswith((".", "!", "?", "ìš”", "ë‹¤", "ì£ ", "ë„¤", "ìŠµë‹ˆë‹¤"))

# --- ì˜¤ë””ì˜¤ ì½œë°± ---
def audio_callback(indata, frames, time_, status):
    if status:
        print(f"[Audio status] {status}")
    try:
        audio_q.put(indata.copy())
    except Exception:
        traceback.print_exc()

# --- ìŒì„± ê°ì§€ ---
def is_speech_chunk(data_chunk, rate=RATE, frame_ms=30):
    """RMS + VAD ê¸°ë°˜ ìŒì„± ê°ì§€"""
    frame_length = int(rate * frame_ms / 1000)
    bytes_data = data_chunk.tobytes()
    speech_frames = 0
    frame_count = 0

    for i in range(0, len(bytes_data), frame_length * 2):
        frame = bytes_data[i:i + frame_length * 2]
        if len(frame) < frame_length * 2:
            break
        frame_count += 1
        try:
            if vad.is_speech(frame, rate):
                speech_frames += 1
        except webrtcvad.Error:
            continue

    return speech_frames > 0

# --- ë©”ì¸ ë£¨í”„ ---
def main_audio_streaming(session_id, socketio, stop_event=None):
    print(f"ğŸ—‚ï¸ ì„¸ì…˜ ì‹œì‘ (ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ): {session_id}")

    buffer = np.zeros((0, 1), dtype=np.int16)
    sentence_buffer = ""
    previous_text = ""
    last_emit_time = time.time()
    silence_counter = 0

    try:
        with sd.InputStream(
            device=INPUT_DEVICE_INDEX,
            samplerate=RATE,
            blocksize=FRAME_SIZE,
            dtype='int16',
            channels=1,
            callback=audio_callback
        ):
            print("ğŸ¤ ì‹¤ì‹œê°„ ìŒì„± ì¸ì‹ ì‹œì‘...")

            while True:
                if stop_event is not None and stop_event.is_set():
                    print("ğŸ›‘ stop_event ìˆ˜ì‹ : ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    break

                if not audio_q.empty():
                    block = audio_q.get()
                    buffer = np.concatenate((buffer, block), axis=0)

                    if len(buffer) >= CHUNK_SIZE:
                        data_chunk = buffer[:CHUNK_SIZE]
                        buffer = buffer[CHUNK_SIZE:]

                        try:
                            # ğŸ”‰ ìŒì„± ê°ì§€
                            if not is_speech_chunk(data_chunk, RATE):
                                silence_counter += 1
                                if silence_counter >= 2 and sentence_buffer.strip():
                                    # âœ… 1.5ì´ˆ ì´ìƒ ë¬´ìŒ â†’ ë¬¸ì¥ ì™„ë£Œë¡œ ê°„ì£¼
                                    kst = timezone(timedelta(hours=9))
                                    now_time = datetime.now(kst).strftime("%H:%M:%S")

                                    translated = translate_text_local(sentence_buffer)
                                    print(f"âœ… ì™„ì„± ë¬¸ì¥: {sentence_buffer}")
                                    print(f"ğŸŒ ë²ˆì—­ ê²°ê³¼: {translated}\n")

                                    socketio.emit('partial_translation', {
                                        'original': sentence_buffer.strip(),
                                        'translated': translated,
                                        'time': now_time,
                                        'session_id': session_id
                                    })

                                    insert_transcript(session_id, sentence_buffer.strip(), translated)
                                    sentence_buffer = ""
                                    previous_text = ""
                                    silence_counter = 0
                                continue
                            else:
                                silence_counter = 0

                            # ğŸ”‰ ë…¸ì´ì¦ˆ ì œê±°
                            reduced = nr.reduce_noise(y=data_chunk.flatten(), sr=RATE)
                            reduced_int16 = np.int16(reduced / np.max(np.abs(reduced)) * 32767)
                            audio_float32 = reduced_int16.astype(np.float32) / 32768.0

                            # ğŸ§  Whisper ì¸ì‹
                            segments, _ = model.transcribe(
                                audio_float32,
                                language=config.LANGUAGE,
                                beam_size=BEAM_SIZE,
                                # --- â­ï¸ í™˜ê°(ì“°ë ˆê¸°ê°’) ì–µì œ ì˜µì…˜ ì¶”ê°€ ---
                                vad_filter=True,  # VAD í•„í„°ë¥¼ ì‚¬ìš©í•´ ìŒì„±ì´ ì—†ëŠ” ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì œê±°
                                no_speech_threshold=0.6,  # ì´ ê°’ ì´í•˜ì˜ 'ìŒì„± í™•ë¥ 'ì€ ë¬´ì‹œ
                                log_prob_threshold=-1.0,  # ì‹ ë¢°ë„ê°€ ë„ˆë¬´ ë‚®ì€ í† í°(ë‹¨ì–´)ì„ ì–µì œ
                                condition_on_previous_text=False  # ì´ì „ í…ìŠ¤íŠ¸ì— ëœ ì˜ì¡´í•˜ì—¬ ë°˜ë³µ í™˜ê°ì„ ì¤„ì„
                            )
                            partial_text = " ".join(seg.text.strip() for seg in segments if seg.text.strip())

                            if partial_text and partial_text != previous_text:
                                # âœ… ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                                new_part = partial_text.replace(previous_text, "").strip()
                                if new_part:
                                    sentence_buffer += " " + new_part
                                    previous_text = partial_text
                                    print(f"ğŸ§© ë¶€ë¶„ ì¸ì‹ ëˆ„ì : {new_part}")

                                # ì¢…ê²°ì–´ë¯¸ ê¸°ë°˜ ë¬¸ì¥ ì™„ì„± ê°ì§€
                                if is_sentence_complete(sentence_buffer):
                                    kst = timezone(timedelta(hours=9))
                                    now_time = datetime.now(kst).strftime("%H:%M:%S")

                                    translated = translate_text_local(sentence_buffer)
                                    print(f"âœ… ì™„ì„± ë¬¸ì¥: {sentence_buffer}")
                                    print(f"ğŸŒ ë²ˆì—­ ê²°ê³¼: {translated}\n")

                                    socketio.emit('partial_translation', {
                                        'original': sentence_buffer.strip(),
                                        'translated': translated,
                                        'time': now_time,
                                        'session_id': session_id
                                    })

                                    insert_transcript(session_id, sentence_buffer.strip(), translated)
                                    sentence_buffer = ""
                                    previous_text = ""

                        except Exception as e:
                            print(f"âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            traceback.print_exc()
                else:
                    time.sleep(0.01)

    except sd.PortAudioError as e:
        print("âŒ ì˜¤ë””ì˜¤ ì¥ì¹˜ ì˜¤ë¥˜:", e)
    except Exception as e:
        print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜:", e)
        traceback.print_exc()
