import os
import torch
import whisperx
from deep_translator import GoogleTranslator
from pyannote.audio import Pipeline
import soundfile as sf
import numpy as np
import pandas as pd

from config import (
    HF_TOKEN,
    DIARIZE_DEVICE,
    DIARIZE_COMPUTE_TYPE,
    DIARIZE_MODEL_TYPE,
    LANGUAGE, # ì‹¤ì‹œê°„ ëª¨ë“œì™€ ë™ì¼í•œ ì–¸ì–´ ì‚¬ìš©
    TARGET_LANG
)

# ============================================
# âš™ï¸ ì„¤ì •
# ============================================
if HF_TOKEN == "ì—¬ê¸°ì—_HUGGINGFACE_TOKEN_ë¶™ì—¬ë„£ê¸°":
    print("="*50)
    print("âš ï¸ [ì„¤ì • ì˜¤ë¥˜] config.py íŒŒì¼ì— HF_TOKENì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
    print("="*50)
# âš ï¸ [í•„ìˆ˜] Hugging Face í† í°ì„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš” (https://huggingface.co/settings/tokens)
# pyannote/speaker-diarization-3.1 ëª¨ë¸ ì ‘ê·¼ì— í•„ìš”í•©ë‹ˆë‹¤.

DEVICE = DIARIZE_DEVICE
COMPUTE_TYPE = DIARIZE_COMPUTE_TYPE
MODEL_TYPE = DIARIZE_MODEL_TYPE

# --- ëª¨ë¸ ìºì‹œ (ì „ì—­ ë³€ìˆ˜) ---
# (app.pyì—ì„œ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œí•  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ìœ ì§€)
model_cache = {
    "whisper": None,
    "align": None,
    "diarize": None
}


# ============================================
# ğŸ”„ ë²ˆì—­ í—¬í¼ í•¨ìˆ˜
# ============================================
def translate_text(text, target=TARGET_LANG):
    """
    ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ëª©í‘œ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
    (audio_processor.pyì˜ í•¨ìˆ˜ì™€ ìœ ì‚¬)
    """
    if not text or not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target=target).translate(text)
    except Exception as e:
        print(f"âš ï¸ (í›„ì²˜ë¦¬) ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return "[ë²ˆì—­ ì‹¤íŒ¨]"


# ============================================
# ğŸš€ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (í•„ìš”ì‹œ í˜¸ì¶œ)
# ============================================

def load_whisper_model():
    """WhisperX STT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if model_cache["whisper"] is None:
        print("ğŸ”„ (í›„ì²˜ë¦¬) WhisperX ëª¨ë¸ ë¡œë“œ ì¤‘... (CPU, ìµœì´ˆ 1íšŒ ì‹œê°„ ì†Œìš”)")
        model_cache["whisper"] = whisperx.load_model(
            MODEL_TYPE,
            DEVICE,
            compute_type=COMPUTE_TYPE,
            language=LANGUAGE
        )
    return model_cache["whisper"]


def load_align_model():
    """WhisperX ì •ë ¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if model_cache["align"] is None:
        print("ğŸ”„ (í›„ì²˜ë¦¬) ì •ë ¬ ëª¨ë¸ ë¡œë“œ ì¤‘... (CPU, ìµœì´ˆ 1íšŒ ì‹œê°„ ì†Œìš”)")
        model_a, metadata = whisperx.load_align_model(
            language_code=LANGUAGE,
            device=DEVICE
        )
        model_cache["align"] = (model_a, metadata)
    return model_cache["align"]


def load_diarize_model():
    """Pyannote í™”ì ë¶„ë¦¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if HF_TOKEN == "ì—¬ê¸°ì—_HUGGINGFACE_TOKEN_ë¶™ì—¬ë„£ê¸°":
        print("âš ï¸ [ì˜¤ë¥˜] HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™”ì ë¶„ë¦¬ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return None

    if model_cache["diarize"] is None:
        print("ğŸ”„ (í›„ì²˜ë¦¬) í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì¤‘... (CPU, ìµœì´ˆ 1íšŒ ì‹œê°„ ì†Œìš”)")
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=HF_TOKEN
            )
            pipeline.to(torch.device(DEVICE))
            model_cache["diarize"] = pipeline
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("Hugging Face í† í°ì´ ìœ íš¨í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
            return None
    return model_cache["diarize"]


# ============================================
# ğŸ™ï¸ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# ============================================

def run_diarization(session_id):
    """
    ì €ì¥ëœ .wav íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ í™”ì ë¶„ë¦¬ ë° ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    (CPUì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤)
    """

    # --- 1. ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸ ---
    # (audio_processor.pyê°€ ì´ ì´ë¦„ìœ¼ë¡œ ì €ì¥í–ˆë‹¤ê³  ê°€ì •)
    audio_file = f"{session_id}.wav"

    if not os.path.exists(audio_file):
        print(f"âŒ (í›„ì²˜ë¦¬) ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {audio_file}")
        return f"[ì˜¤ë¥˜] ì„¸ì…˜ ì˜¤ë””ì˜¤ íŒŒì¼({audio_file})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    print(f"âœ… (í›„ì²˜ë¦¬) ì„¸ì…˜ '{session_id}' ë¶„ì„ ì‹œì‘... (CPU ì‚¬ìš©, ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŒ)")

    try:
        # --- 2. ì˜¤ë””ì˜¤ ë¡œë“œ ---
        # (WhisperXëŠ” 16kHz ëª¨ë…¸ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤)
        audio_data, sr = sf.read(audio_file, dtype='float32')
        if audio_data.ndim > 1:  # ìŠ¤í…Œë ˆì˜¤ -> ëª¨ë…¸
            audio_data = np.mean(audio_data, axis=1)
        if sr != 16000:
            # (resampling ë¡œì§ì´ í•„ìš”í•˜ì§€ë§Œ, audio_processor.pyê°€ 16kHzë¡œ ì €ì¥í–ˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥)
            print(f"âš ï¸ ê²½ê³ : ì˜¤ë””ì˜¤ ìƒ˜í”Œë ˆì´íŠ¸ê°€ 16kHzê°€ ì•„ë‹™ë‹ˆë‹¤. ({sr}Hz)")
            # ê°„ë‹¨í•œ ë‹¤ìš´ìƒ˜í”Œë§ (ê¶Œì¥: librosa ì‚¬ìš©)
            if sr > 16000:
                step = sr // 16000
                audio_data = audio_data[::step]

    except Exception as e:
        print(f"âŒ (í›„ì²˜ë¦¬) ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "[ì˜¤ë¥˜] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    final_transcript = []

    try:
        # --- 3. Whisper STT ì‹¤í–‰ ---
        print("ğŸ”„ (1/4) ìŒì„± ì¸ì‹(STT) ì‹¤í–‰ ì¤‘...")
        model = load_whisper_model()
        result = model.transcribe(audio_data, batch_size=4)  # CPU ë°°ì¹˜ í¬ê¸°

        # --- 4. ì •ë ¬ ëª¨ë¸ ì‹¤í–‰ (ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„) ---
        print("ğŸ”„ (2/4) íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬ ì¤‘...")
        align_model, metadata = load_align_model()
        result = whisperx.align(
            result["segments"],
            align_model,
            metadata,
            audio_data,
            DEVICE,
            return_char_alignments=False
        )

        # --- 5. í™”ì ë¶„ë¦¬ ëª¨ë¸ ì‹¤í–‰ ---
        print("ğŸ”„ (3/4) í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘...")
        diarize_model = load_diarize_model()

        if diarize_model is None:
            # í™”ì ë¶„ë¦¬ ì‹¤íŒ¨ ì‹œ, í™”ì ì—†ì´ í…ìŠ¤íŠ¸ë§Œ ë²ˆì—­
            print("âš ï¸ (3/4) í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì¼ë°˜ ë²ˆì—­ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            for segment in result["segments"]:
                text = segment.get("text", "").strip()
                if text:
                    translated = translate_text(text)
                    final_transcript.append(f"**[ë‚´ìš©]**: {text}\n*({translated})*\n")
            return "\n".join(final_transcript)

        # Pyannoteê°€ íŒŒì¼ ê²½ë¡œë¥¼ í•„ìš”ë¡œ í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ audio_data ëŒ€ì‹  audio_file ì‚¬ìš©
        diarize_result = diarize_model(audio_file)

        # â­ï¸ [ì‹ ê·œ] pyannote 3.x í˜¸í™˜ì„± í•´ê²° (KeyError: 'e' ìˆ˜ì •)
        # whisperxê°€ Annotation ê°ì²´ë¥¼ ì˜ëª» íŒŒì‹±í•˜ë¯€ë¡œ, ìˆ˜ë™ìœ¼ë¡œ DataFrame ë³€í™˜
        print("ğŸ”„ (3.5/4) í™”ì ë¶„ë¦¬ ê²°ê³¼ í¬ë§· ë³€í™˜ ì¤‘...")
        diarize_segments = []
        for segment, track, speaker in diarize_result.itertracks(yield_label=True):
            diarize_segments.append({
                'start': segment.start,
                'end': segment.end,
                'speaker': speaker
            })

        if not diarize_segments:
            print("âš ï¸ (í›„ì²˜ë¦¬) í™”ì ë¶„ë¦¬ ëª¨ë¸ì´ ì•„ë¬´ë„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ë²ˆì—­ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            # (fallback to no-speaker logic)
            for segment in result["segments"]:
                text = segment.get("text", "").strip()
                if text:
                    translated = translate_text(text)
                    final_transcript.append(f"**[ë‚´ìš©]**: {text}\n*({translated})*\n")
            return "\n".join(final_transcript)

        # â­ï¸ ìˆ˜ë™ìœ¼ë¡œ ë³€í™˜í•œ DataFrame ìƒì„±
        diarize_df = pd.DataFrame(diarize_segments)
        # â­ï¸ [ì‹ ê·œ] ì—¬ê¸°ê¹Œì§€ ---

        # --- 6. STT ê²°ê³¼ì™€ í™”ì ë¶„ë¦¬ ê²°ê³¼ ë³‘í•© ---
        print("ğŸ”„ (4/4) í™”ìì™€ í…ìŠ¤íŠ¸ ë³‘í•© ì¤‘...")
        final_result = whisperx.assign_word_speakers(diarize_df, result)

        # --- 7. ê²°ê³¼ í¬ë§·íŒ… ë° ë²ˆì—­ ---
        print("âœ… ë¶„ì„ ì™„ë£Œ. ìµœì¢… í…ìŠ¤íŠ¸ í¬ë§·íŒ… ë° ë²ˆì—­ ì¤‘...")

        current_speaker = None
        current_transcript = ""

        for segment in final_result["segments"]:
            speaker = segment.get("speaker", "UNKNOWN")
            text = segment.get("text", "").strip()

            if not text:
                continue

            if speaker == current_speaker:
                # ê°™ì€ í™”ìê°€ ë§ì„ ì´ì–´ê°€ëŠ” ê²½ìš°
                current_transcript += " " + text
            else:
                # í™”ìê°€ ë°”ë€ ê²½ìš° (ì´ì „ ëŒ€í™” ì €ì¥)
                if current_speaker is not None and current_transcript:
                    translated = translate_text(current_transcript)
                    final_transcript.append(f"**{current_speaker}**: {current_transcript}\n*({translated})*\n")

                # ìƒˆ í™”ìë¡œ ì´ˆê¸°í™”
                current_speaker = speaker
                current_transcript = text

        # ë§ˆì§€ë§‰ ëŒ€í™” ì €ì¥
        if current_speaker is not None and current_transcript:
            translated = translate_text(current_transcript)
            final_transcript.append(f"**{current_speaker}**: {current_transcript}\n*({translated})*\n")

        if not final_transcript:
            return "[ë¶„ì„ ê²°ê³¼] ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        return "\n".join(final_transcript)

    except Exception as e:
        print(f"âŒ (í›„ì²˜ë¦¬) ë¶„ì„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return f"[ì˜¤ë¥˜] ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨: {e}"


# ============================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ìš© (ì§ì ‘ ì‹¤í–‰ ì‹œ)
# ============================================
if __name__ == "__main__":
    # ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ë•Œ í…ŒìŠ¤íŠ¸í•  ì„¸ì…˜ ID (ì˜ˆ: "2025-11-11_11:38.wav" íŒŒì¼ì´ ìˆì–´ì•¼ í•¨)
    TEST_SESSION_ID = "diarizeTest"

    if HF_TOKEN == "ì—¬ê¸°ì—_HUGGINGFACE_TOKEN_ë¶™ì—¬ë„£ê¸°":
        print("=" * 50)
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: HF_TOKENì„ `config.py` íŒŒì¼ ìƒë‹¨ì— ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 50)
    elif not os.path.exists(f"{TEST_SESSION_ID}.wav"):
        print("=" * 50)
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: '{TEST_SESSION_ID}.wav' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ í”„ë¡œì íŠ¸ í´ë”ì— ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        print("=" * 50)
    else:
        print(f"=== '{TEST_SESSION_ID}' í™”ì ë¶„ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        result = run_diarization(TEST_SESSION_ID)
        print("\n=== âœ¨ ìµœì¢… ê²°ê³¼ ===\n")
        print(result)
        print("\n=== í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ===")