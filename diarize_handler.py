import os
import torch
import whisperx
from deep_translator import GoogleTranslator
from pyannote.audio import Pipeline
import soundfile as sf
import numpy as np
import pandas as pd  # â­ï¸ 'pd' not defined ì˜¤ë¥˜ ìˆ˜ì •ì„ ìœ„í•œ ì„í¬íŠ¸

# â­ï¸ configì—ì„œ ì„¤ì •ê°’ ì„í¬íŠ¸
from config import (
    HF_TOKEN,
    DIARIZE_DEVICE,
    DIARIZE_COMPUTE_TYPE,
    DIARIZE_MODEL_TYPE,
    LANGUAGE,  # ì‹¤ì‹œê°„ ëª¨ë“œì™€ ë™ì¼í•œ ì–¸ì–´ ì‚¬ìš©
    TARGET_LANG
)

# ============================================
# âš™ï¸ ì„¤ì •
# ============================================

# âš ï¸ [í•„ìˆ˜] Hugging Face í† í° í™•ì¸ (DEFAULT_TOKEN_PLHëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤)
if HF_TOKEN == "DEFAULT_TOKEN_PLH" or not HF_TOKEN:
    print("=" * 50)
    print("âš ï¸ [ì„¤ì • ì˜¤ë¥˜] config.py íŒŒì¼ì— HF_TOKENì„ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
    print("=" * 50)

# â­ï¸ config.pyì—ì„œ ê°€ì ¸ì˜¨ ê°’ìœ¼ë¡œ ë³€ìˆ˜ ì„¤ì •
DEVICE = DIARIZE_DEVICE
COMPUTE_TYPE = DIARIZE_COMPUTE_TYPE
MODEL_TYPE = DIARIZE_MODEL_TYPE
# (LANGUAGEì™€ TARGET_LANGëŠ” ì´ë¯¸ ì„í¬íŠ¸ë¨)


# --- ëª¨ë¸ ìºì‹œ (ì „ì—­ ë³€ìˆ˜) ---
model_cache = {
    "whisper": None,
    "align": None,
    "diarize": None
}


# ============================================
# ğŸ”„ ë²ˆì—­ í—¬í¼ í•¨ìˆ˜
# ============================================
def translate_text(text, target=TARGET_LANG):
    """
    ì…ë ¥ëœ í…ìŠ¤íŠ¸ë¥¼ ëª©í‘œ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
    """
    if not text or not text.strip():
        return ""
    try:
        return GoogleTranslator(source='auto', target=target).translate(text)
    except Exception as e:
        print(f"âš ï¸ (í›„ì²˜ë¦¬) ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return "[ë²ˆì—­ ì‹¤íŒ¨]"


# ============================================
# ğŸš€ ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜ (í•„ìš”ì‹œ í˜¸ì¶œ)
# ============================================

def load_whisper_model():
    """WhisperX STT ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if model_cache["whisper"] is None:
        print("ğŸ”„ (í›„ì²˜ë¦¬) WhisperX ëª¨ë¸ ë¡œë“œ ì¤‘... (CPU, ìµœì´ˆ 1íšŒ ì‹œê°„ ì†Œìš”)")
        model_cache["whisper"] = whisperx.load_model(
            MODEL_TYPE,
            DEVICE,
            compute_type=COMPUTE_TYPE,
            language=LANGUAGE
        )
    return model_cache["whisper"]


def load_align_model():
    """WhisperX ì •ë ¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if model_cache["align"] is None:
        print("ğŸ”„ (í›„ì²˜ë¦¬) ì •ë ¬ ëª¨ë¸ ë¡œë“œ ì¤‘... (CPU, ìµœì´ˆ 1íšŒ ì‹œê°„ ì†Œìš”)")
        model_a, metadata = whisperx.load_align_model(
            language_code=LANGUAGE,
            device=DEVICE
        )
        model_cache["align"] = (model_a, metadata)
    return model_cache["align"]


def load_diarize_model():
    """Pyannote í™”ì ë¶„ë¦¬ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not HF_TOKEN or HF_TOKEN == "DEFAULT_TOKEN_PLH":
        print("âš ï¸ [ì˜¤ë¥˜] HF_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í™”ì ë¶„ë¦¬ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤.")
        return None

    if model_cache["diarize"] is None:
        print("ğŸ”„ (í›„ì²˜ë¦¬) í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì¤‘... (CPU, ìµœì´ˆ 1íšŒ ì‹œê°„ ì†Œìš”)")
        try:
            pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
                use_auth_token=HF_TOKEN
            )
            pipeline.to(torch.device(DEVICE))
            model_cache["diarize"] = pipeline
        except Exception as e:
            print(f"âš ï¸ í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("Hugging Face í† í°ì´ ìœ íš¨í•œì§€, Gated Model ì•½ê´€ì— ë™ì˜í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return None
    return model_cache["diarize"]


# ============================================
# ğŸ™ï¸ ë©”ì¸ ë¶„ì„ í•¨ìˆ˜
# ============================================

def run_diarization(session_id):
    """
    ì €ì¥ëœ .wav íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ í™”ì ë¶„ë¦¬ ë° ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    (CPUì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤)
    """

    # --- 1. ì˜¤ë””ì˜¤ íŒŒì¼ í™•ì¸ ---
    output_dir = "wav"
    audio_file = os.path.join(output_dir, f"{session_id}.wav")

    if not os.path.exists(audio_file):
        print(f"âŒ (í›„ì²˜ë¦¬) ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {audio_file}")
        return f"[ì˜¤ë¥˜] ì„¸ì…˜ ì˜¤ë””ì˜¤ íŒŒì¼({audio_file})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    print(f"âœ… (í›„ì²˜ë¦¬) ì„¸ì…˜ '{session_id}' ë¶„ì„ ì‹œì‘... (CPU ì‚¬ìš©, ë§¤ìš° ëŠë¦´ ìˆ˜ ìˆìŒ)")

    try:
        # --- 2. ì˜¤ë””ì˜¤ ë¡œë“œ ---
        audio_data, sr = sf.read(audio_file, dtype='float32')
        if audio_data.ndim > 1:
            audio_data = np.mean(audio_data, axis=1)
        if sr != 16000:
            print(f"âš ï¸ ê²½ê³ : ì˜¤ë””ì˜¤ ìƒ˜í”Œë ˆì´íŠ¸ê°€ 16kHzê°€ ì•„ë‹™ë‹ˆë‹¤. ({sr}Hz). ë¦¬ìƒ˜í”Œë§ ì‹œë„...")
            if sr > 16000:
                step = int(sr / 16000)
                audio_data = audio_data[::step]

    except Exception as e:
        print(f"âŒ (í›„ì²˜ë¦¬) ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return "[ì˜¤ë¥˜] ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

    final_transcript = []

    try:
        # --- 3. Whisper STT ì‹¤í–‰ ---
        print("ğŸ”„ (1/4) ìŒì„± ì¸ì‹(STT) ì‹¤í–‰ ì¤‘...")
        model = load_whisper_model()
        result = model.transcribe(audio_data, batch_size=4)

        # --- 4. ì •ë ¬ ëª¨ë¸ ì‹¤í–‰ (ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„) ---
        print("ğŸ”„ (2/4) íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬ ì¤‘...")
        align_model, metadata = load_align_model()
        result = whisperx.align(
            result["segments"],
            align_model,
            metadata,
            audio_data,
            DEVICE,
            return_char_alignments=False
        )

        # --- 5. í™”ì ë¶„ë¦¬ ëª¨ë¸ ì‹¤í–‰ ---
        print("ğŸ”„ (3/4) í™”ì ë¶„ë¦¬ ì‹¤í–‰ ì¤‘...")
        diarize_model = load_diarize_model()

        if diarize_model is None:
            print("âš ï¸ (3/4) í™”ì ë¶„ë¦¬ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ì¼ë°˜ ë²ˆì—­ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            for segment in result["segments"]:
                text = segment.get("text", "").strip()
                if text:
                    translated = translate_text(text)
                    final_transcript.append(f"**[ë‚´ìš©]**: {text}\n*({translated})*\n")
            return "\n".join(final_transcript)

        diarize_result = diarize_model(audio_file)

        print("ğŸ”„ (3.5/4) í™”ì ë¶„ë¦¬ ê²°ê³¼ í¬ë§· ë³€í™˜ ì¤‘...")
        diarize_segments = []
        for segment, track, speaker in diarize_result.itertracks(yield_label=True):
            diarize_segments.append({
                'start': segment.start,
                'end': segment.end,
                'speaker': speaker
            })

        if not diarize_segments:
            print("âš ï¸ (í›„ì²˜ë¦¬) í™”ì ë¶„ë¦¬ ëª¨ë¸ì´ ì•„ë¬´ë„ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ ë²ˆì—­ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            for segment in result["segments"]:
                text = segment.get("text", "").strip()
                if text:
                    translated = translate_text(text)
                    final_transcript.append(f"**[ë‚´ìš©]**: {text}\n*({translated})*\n")
            return "\n".join(final_transcript)

        diarize_df = pd.DataFrame(diarize_segments)

        # --- 6. STT ê²°ê³¼ì™€ í™”ì ë¶„ë¦¬ ê²°ê³¼ ë³‘í•© ---
        print("ğŸ”„ (4/4) í™”ìì™€ í…ìŠ¤íŠ¸ ë³‘í•© ì¤‘...")
        final_result = whisperx.assign_word_speakers(diarize_df, result)

        # --- 7. ê²°ê³¼ í¬ë§·íŒ… ë° ë²ˆì—­ ---
        print("âœ… ë¶„ì„ ì™„ë£Œ. ìµœì¢… í…ìŠ¤íŠ¸ í¬ë§·íŒ… ë° ë²ˆì—­ ì¤‘...")

        # â­ï¸ [ìˆ˜ì •] ìš”ì²­ëŒ€ë¡œ "ë¬¸ì¥ë³„"ë¡œ ì›ë¬¸/ë²ˆì—­ì„ ë‚˜ëˆ„ë„ë¡ ë¡œì§ ë³€ê²½
        # (ì´ì „ì˜ 'current_speaker'ì™€ í•©ì¹˜ëŠ” ë¡œì§ ì œê±°)
        for segment in final_result["segments"]:
            speaker = segment.get("speaker", "UNKNOWN")
            text = segment.get("text", "").strip()

            if not text:
                continue

            # â­ï¸ ê° ë¬¸ì¥ë³„ë¡œ ë°”ë¡œ ë²ˆì—­ ì‹¤í–‰
            translated = translate_text(text)

            # â­ï¸ ì›ë¬¸(í™”ìí¬í•¨), ë²ˆì—­, ë¹ˆ ì¤„ ìˆœì„œë¡œ ì¶”ê°€
            final_transcript.append(f"**{speaker}**: {text}")
            final_transcript.append(f"*({translated})*")
            final_transcript.append("")  # ì¤„ë°”ê¿ˆìš© ë¹ˆ ì¤„

        if not final_transcript:
            return "[ë¶„ì„ ê²°ê³¼] ì¸ì‹ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        return "\n".join(final_transcript)

    except Exception as e:
        print(f"âŒ (í›„ì²˜ë¦¬) ë¶„ì„ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return f"[ì˜¤ë¥˜] ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨: {e}"


# ============================================
# ğŸ§ª í…ŒìŠ¤íŠ¸ìš© (ì§ì ‘ ì‹¤í–‰ ì‹œ)
# ============================================
if __name__ == "__main__":
    TEST_SESSION_ID = "diarizeTest"
    test_audio_file = os.path.join("wav", f"{TEST_SESSION_ID}.wav")

    if HF_TOKEN == "DEFAULT_TOKEN_PLH" or not HF_TOKEN:
        print("=" * 50)
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: config.py íŒŒì¼ì— HF_TOKENì„ ì…ë ¥í•˜ì„¸ìš”.")
        print("=" * 50)
    elif not os.path.exists(test_audio_file):
        print("=" * 50)
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: '{test_audio_file}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì˜¤ë””ì˜¤ íŒŒì¼ì„ 'wav' í´ë”ì— ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        print("=" * 50)
    else:
        print(f"=== '{TEST_SESSION_ID}' í™”ì ë¶„ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        result = run_diarization(TEST_SESSION_ID)
        print("\n=== âœ¨ ìµœì¢… ê²°ê³¼ ===\n")
        print(result)
        print("\n=== í…ŒìŠ¤íŠ¸ ì¢…ë£Œ ===")